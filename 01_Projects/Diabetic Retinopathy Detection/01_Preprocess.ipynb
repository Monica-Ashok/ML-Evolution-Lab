{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Automated Diabetic Retinopathy Screening Proof-of-Concept (PoC)\n",
    "\n",
    "### üéØ **Objective**\n",
    "The goal of this initial phase is to **scan the raw dataset and create a clean, reliable metadata file**. This file will serve as the single source of truth for our project, mapping every valid image to its correct diagnosis and ensuring the integrity of our dataset by removing duplicates and corrupted files.\n",
    "\n",
    "This \"metadata-first\" approach is highly efficient as it avoids loading large images into memory. Instead, we create a lightweight map that will guide all future processing and modeling steps.\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ **Business Rationale (Senior BA Perspective)**\n",
    "As the lead for this PoC, my primary goal is to de-risk the project early. Before we invest significant computational resources into image processing and model training, we must first **validate and audit our primary asset: the data.**\n",
    "\n",
    "An unreliable dataset leads to untrustworthy models. By creating a clean metadata file, we establish a foundational layer of data governance. This ensures that our subsequent analyses are reproducible, traceable, and built upon a verified and complete dataset. This step is non-negotiable for building a robust, production-ready AI solution in a regulated medical environment.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ **Dataset**\n",
    "We are using the raw data from the [Kaggle APTOS 2019 Blindness Detection](https://www.kaggle.com/competitions/aptos2019-blindness-detection/data) competition. The key components are:\n",
    "- `train_images/`: A folder containing all raw images.\n",
    "- `train.csv`: A manifest file linking image IDs to their diagnoses.\n",
    "\n",
    "#### Conclusion & Next Steps\n",
    "This phase successfully transformed a raw, inconsistent collection of images into a standardized, analysis-ready dataset. By creating this repeatable preprocessing pipeline, we have not only curated the data for this PoC but have also developed a template for ingesting future clinical data. **The successful completion of this phase provides the high-quality foundation necessary to proceed with exploratory analysis**."
   ],
   "id": "bd9845fcf22deed0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image  # Pillow is used for its efficient image verification capabilities.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ],
   "id": "cb62e79362ed5b2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load and Prepare Initial Metadata\n",
    "\n",
    "Our first action is to load the `train.csv` manifest provided with the dataset. We will then enrich this data by creating a full, verifiable path to each image file."
   ],
   "id": "e465c6def6586470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BASE_DIR = '.'\n",
    "TRAIN_IMG_DIR = os.path.join(BASE_DIR, 'Data/aptos2019-blindness-detection/train_images')\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, 'Data/aptos2019-blindness-detection/train.csv')\n",
    "\n",
    "# Load the CSV\n",
    "try:\n",
    "    df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    print(f\"Successfully loaded {TRAIN_CSV_PATH}. Found metadata for {len(df)} images.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[FATAL] Could not find {TRAIN_CSV_PATH}. Please ensure it is in the correct directory.\")\n",
    "    df = pd.DataFrame() # Create an empty dataframe to avoid downstream errors\n",
    "\n",
    "# Create the full filepath for each image. This is a crucial step to make our\n",
    "# metadata self-contained and directly usable.\n",
    "if not df.empty:\n",
    "    df['filepath'] = df['id_code'].apply(lambda x: os.path.join(TRAIN_IMG_DIR, f\"{x}.png\"))\n",
    "    print(\"\\nSample of the initial DataFrame with filepaths:\")\n",
    "    print(df.head())"
   ],
   "id": "17de77ac217c65d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Analyze Class Distribution\n",
    "\n",
    "Before cleaning the data, it's essential to understand its composition. A significant class imbalance‚Äîwhere some diagnoses are much more common than others‚Äîis a critical finding that will heavily influence our modeling strategy later on."
   ],
   "id": "8a98c8b5562d5566"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Rationale: A visual inspection of the class distribution provides an immediate and clear understanding of the dataset's imbalance. This informs stakeholders and the modeling team of the need for mitigation strategies like class weighting or stratified sampling.\n",
    "\n",
    "if not df.empty:\n",
    "    # Map numeric diagnosis to human-readable labels for plotting\n",
    "    diagnosis_map = {\n",
    "        0: 'No DR',\n",
    "        1: 'Mild',\n",
    "        2: 'Moderate',\n",
    "        3: 'Severe',\n",
    "        4: 'Proliferative DR'\n",
    "    }\n",
    "    df['diagnosis_label'] = df['diagnosis'].map(diagnosis_map)\n",
    "\n",
    "    # Calculate counts\n",
    "    class_counts = df['diagnosis_label'].value_counts().sort_index()\n",
    "    print(\"Class Distribution:\")\n",
    "    print(class_counts)\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=class_counts.index, y=class_counts.values, hue=class_counts.index, palette=\"viridis\", legend=False)\n",
    "    plt.title('Distribution of Diabetic Retinopathy Classes')\n",
    "    plt.xlabel('Diagnosis')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ],
   "id": "c1d93e2234ee11b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Verify Data Integrity (Check for Corrupted & Duplicate Images)\n",
    "\n",
    "This is a critical data governance step. We will iterate through every file path in our DataFrame to ensure it points to a valid, readable, and unique image.\n",
    "- **Corrupted Files:** We will use Pillow's `Image.verify()` method, which is a fast way to check if a file is a valid image without loading all pixel data.\n",
    "- **Duplicate Files:** We will compute an MD5 hash (a unique digital fingerprint) for each file. By tracking these hashes, we can identify and discard any exact duplicates."
   ],
   "id": "2a273585bfa909df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not df.empty:\n",
    "    def get_file_hash(filepath):\n",
    "        \"\"\"Calculates the MD5 hash of a file for duplicate detection.\"\"\"\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "    valid_images = []\n",
    "    hashes = set()\n",
    "\n",
    "    # Using tqdm for a progress bar\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Verifying Images\"):\n",
    "        filepath = row['filepath']\n",
    "        try:\n",
    "            # 1. Check if the file exists\n",
    "            if not os.path.exists(filepath):\n",
    "                print(f\"File not found, skipping: {filepath}\")\n",
    "                continue\n",
    "\n",
    "            # 2. Check if it's a valid image that can be opened and verified\n",
    "            img = Image.open(filepath)\n",
    "            img.verify()\n",
    "\n",
    "            # 3. Check for duplicates using hashing\n",
    "            file_hash = get_file_hash(filepath)\n",
    "            if file_hash in hashes:\n",
    "                continue  # Skip if it's a duplicate\n",
    "\n",
    "            hashes.add(file_hash)\n",
    "            valid_images.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping corrupted or problematic file: {filepath} due to {e}\")\n",
    "\n",
    "    # Create a new, clean DataFrame\n",
    "    clean_df = pd.DataFrame(valid_images)\n",
    "\n",
    "    print(f\"\\nOriginal image count: {len(df)}\")\n",
    "    print(f\"Clean image count: {len(clean_df)}\")\n",
    "    print(f\"Removed {len(df) - len(clean_df)} missing, corrupted, or duplicate images.\")"
   ],
   "id": "bb9949d086b9e6c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Save Clean Metadata\n",
    "\n",
    "This is the **final and most important step** of this notebook. We will now serialize our `clean_df` DataFrame to a CSV file. This single, clean file will serve as the starting point for all future notebooks (EDA, processing, and modeling), ensuring a consistent and reproducible workflow."
   ],
   "id": "9d8e497d0e65590a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if 'clean_df' in locals() and not clean_df.empty:\n",
    "    output_path = os.path.join(BASE_DIR, \"Data/aptos2019-blindness-detection/clean_metadata.csv\")\n",
    "    clean_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Clean metadata successfully saved to: {output_path}\")\n",
    "    print(\"Sample of the final, clean DataFrame:\")\n",
    "    print(clean_df.head())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Clean DataFrame not created or is empty. Skipping save.\")"
   ],
   "id": "7f203e59e386b234"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
